本文介绍K近邻算法在R语言中如何使用。

数据集，采用R语言内置的iris数据集。

#查看数据集前六个观测
head(iris)



K近邻算法R语言实践
###############################################################################################



第一步：数据集分为训练集和测试集
###############################################################################################

index <-sample(1:nrow(iris), 100)
iris.train <-iris[index, ]
iris.test <-iris[-index, ]


第二步：加载能够做k近邻的class包
###############################################################################################

library(class)


第三步：利用kNN算法对测试集进行分类
###############################################################################################

result.KNN <-knn(train=subset(iris.train,select=-Species), test=subset(iris.test,select=-Species), cl=iris.train$Species)


第四步：生成结果集的混淆矩阵
###############################################################################################
table(result.KNN, iris.test$Species)





kNN算法原理

1、K最近邻(k-NearestNeighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。
该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，
则该样本也属于这个类别。



2、KNN算法中，所选择的邻居都是已经正确分类的对象。
该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。
KNN方法虽然从原理上也依赖于极限定理，但在类别决策时，只与极少量的相邻样本有关。
由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，
因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合



3、KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将
这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。
更有用的方法是将不同距离的邻居对该样本产生的影响给予不同的权值(weight)，如权值与距离成正比。



